{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\torch\\__init__.py:749: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append(\"\\\\\".join(os.getcwd().split(\"\\\\\")[:-1]))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scraper import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_github_repo = \"https://github.com/FHNW-IVGI/Geoharvester/blob/main/scraper/data/\"\n",
    "url_github_repo_suffix = \"?raw=true\"\n",
    "url_geoservices_CH_csv = \"{}geoservices_CH.csv{}\".format(url_github_repo,url_github_repo_suffix)\n",
    "fields_to_include = [\"provider\",\"title\", \"keywords\", \"abstract\", \"service\", \"endpoint\", \"preview\"]\n",
    "\n",
    "def load_data(rawdata=False):\n",
    "    if rawdata:\n",
    "        dataframe = pd.read_csv(url_geoservices_CH_csv, low_memory=False)\n",
    "    else:\n",
    "        dataframe = pd.read_csv(url_geoservices_CH_csv, usecols=fields_to_include, low_memory=False)\n",
    "    return dataframe\n",
    "\n",
    "# dataframe_some_cols = load_data()\n",
    "# database = dataframe_some_cols.fillna(\"nan\")\n",
    "dataframe_some_cols = load_data(rawdata=True)\n",
    "database_raw = dataframe_some_cols.fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"./data/geoservices_CH.csv\", low_memory=False)\n",
    "def count(row, exc=0):\n",
    "    try:\n",
    "        num = len(str(row).split())\n",
    "    except TypeError:\n",
    "        num = exc\n",
    "    except AttributeError:\n",
    "        num = exc\n",
    "    if pd.isna(row):\n",
    "        num = 0\n",
    "    return num\n",
    "db['abstract_w_count'] = db['ABSTRACT'].apply(count)\n",
    "db['keywords_w_count'] = db['KEYWORDS'].apply(count)\n",
    "db.replace(['Bund','FL_LI','Geodienste','KT_AG','KT_AI','KT_AR','KT_BE','KT_BL','KT_BS','KT_FR','KT_GE','KT_GL','KT_GR','KT_JU','KT_SG','KT_SH','KT_SO','KT_SZ','KT_TG','KT_TI','KT_UR','KT_VD','KT_ZG','KT_ZH'],\n",
    "              ['Bund','LI','Geodienste','AG','AI','AR','BE','BL','BS','FR','GE','GL','GR','JU','SG','SH','SO','SZ','TG','TI','UR','VD','ZG','ZH'], inplace=True)\n",
    "db_nan = db[db['OWNER'] == 'Bund'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[(db['OWNER'] == 'AI') & (db['keywords_w_count'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in db.columns.to_list():\n",
    "    if (db[column][db[column].isna()]).empty:\n",
    "        print(f\"No field missing in {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gr = db.groupby(['OWNER']).count()[['TITLE','ABSTRACT', 'KEYWORDS', 'NAME']]\n",
    "db_gr['abstract_perc'] = 100 / db_gr['TITLE'] * db_gr['ABSTRACT']\n",
    "db_gr['keywords_perc'] = 100 / db_gr['TITLE'] * db_gr['KEYWORDS']\n",
    "db_gr['name_perc'] = 100 / db_gr['TITLE'] * db_gr['NAME']\n",
    "db_gr.rename({'Geodienste':'Geodnst'}, inplace=True)\n",
    "db_gr.sort_values(by=['OWNER'], ascending=False, inplace=True)\n",
    "db_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, db_gr['TITLE'])\n",
    "plt.title(\"Number of OWS\")\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.barh(db_gr.index, db_gr['abstract_perc'])\n",
    "plt.title(\"Abstact filled %\", fontsize=22)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":18})\n",
    "plt.xlim(0,100)\n",
    "# plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.barh(db_gr.index, db_gr['keywords_perc'], )\n",
    "plt.title(\"Keywords filled %\", fontsize=22)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":18})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\", fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 / db['abstract_w_count'].count() * db[db['abstract_w_count'] > 20]['ABSTRACT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['abstract_w_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = db[['ABSTRACT', 'KEYWORDS', 'OWNER', 'abstract_w_count','keywords_w_count', 'TITLE']]\n",
    "test = test.groupby(['OWNER']).mean()[['abstract_w_count','keywords_w_count']]\n",
    "test.rename({'Geodienste':'Geodnst'}, inplace=True)\n",
    "test.sort_values(by=['OWNER'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.groupby(['OWNER','keywords_w_count']).count()['TITLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(test.index, test['abstract_w_count'], )\n",
    "plt.title(\"Abstract Wordcount Median\")\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "# plt.vlines(x=1.0, ymin=0, ymax=len(test.index.to_list())-0.5, color='grey')\n",
    "plt.barh(test.index, test['keywords_w_count'], )\n",
    "plt.title(\"Keywords average word count\", fontsize=22)\n",
    "plt.autoscale(tight=True)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.rcParams.update({\"font.size\":18})\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions from utils.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement of the TF-IDF with BM25 to execute queries on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstracts = result['abstract'].values.tolist()\n",
    "bm25 = utils.TFIDF_BM25()\n",
    "bm25.cleansing_ranking(database, column='abstract') # 1421 lines in 36 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vector to the database\n",
    "bm25.fit()\n",
    "# search the best match in the vector\n",
    "res = bm25.search('Brandmeldeanlage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the keyword extraction with spacy (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = utils.NLP_spacy()\n",
    "# Keyword\n",
    "keywords_NLP = NLP.extract_keywords(database, column='abstract') # with small models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_NLP[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original abstract\n",
    "database['abstract'].values.tolist()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LSA and LSI with gensim for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSA = utils.LSI_LDA()\n",
    "abstracts = LSA.preprocess(database)\n",
    "LSA.compute_coherence_values_LSI((4,16,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_number_topics = 12\n",
    "number_of_words = 20\n",
    "lsamodel = LSA.create_gensim_lsa_model(best_number_topics, number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsamodel.print_topics(best_number_topics, number_of_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Text Summarization and Latent Dirchlet allocation (LDA) for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/text-summarization-for-clustering-documents-2e074da6437a\n",
    "# https://towardsdatascience.com/nlp-topic-modeling-to-identify-clusters-ca207244d04f\n",
    "\n",
    "# https://medium.com/plain-simple-software/build-an-ai-text-summarizer-in-python-6209fb23875d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = utils.NLP_spacy()\n",
    "keywords_dataset = NLP.extract_refined_keywords(database, use_rake=True, column='abstract',\n",
    "                                          keyword_length=3, num_keywords=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.loc[3]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the extracted keywords to the raw data dataframe\n",
    "def join_keywords(keywords_list):\n",
    "    keywords = ', '.join(kw for kw in keywords_list)\n",
    "    return keywords\n",
    "database_raw['keywords_nlp'] = list(map(join_keywords, keywords_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.title(\"Summarizer\")\n",
    "input_text = st.text_area(label='Enter full text:', value='', height=250)\n",
    "st.button(\"submit\")\n",
    "output_text = st.text_area(label='Summarized text:', value='', height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv('OPENAI_KEY') # WARNING: limits to 18$ free credits!\n",
    "\n",
    "def summarize_GPT(text):\n",
    "    prompt = f\"summarize this text: {text}\"\n",
    "    model = openai.Completion.create(model='text-davinci-003', prompt=prompt, temperature=.5, max_tokens=1000,)\n",
    "    return model[\"choices\"][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_GPT('''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. Hence, owing to the fact\n",
    "that it is a very innovative and, above all, safe solution, it is possible to determine the condition of the\n",
    "building, locate places exposed to thermal escape, and plan actions to improve the condition of the\n",
    "facility. The presented work is devoted to the technology of creating a dense point cloud and a 3D\n",
    "model, based on data obtained from UAV. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5 abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/mt5-large\") #  google/mt5-large\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/mt5-large')\n",
    "device = torch.device('cpu')\n",
    "task_prefix = 'Summarize:'\n",
    "body = '''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. Hence, owing to the fact\n",
    "that it is a very innovative and, above all, safe solution, it is possible to determine the condition of the\n",
    "building, locate places exposed to thermal escape, and plan actions to improve the condition of the\n",
    "facility. The presented work is devoted to the technology of creating a dense point cloud and a 3D\n",
    "model, based on data obtained from UAV. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "tokenized_text = tokenizer.encode(task_prefix + body.strip().replace('\\n',''), return_tensors=\"pt\").to(device)\n",
    "summary = model.generate(tokenized_text, num_beams=1, no_repeat_ngram_size=1, min_length=20, max_length=80,\n",
    "                         early_stopping=True)\n",
    "output = tokenizer.decode(summary[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstractive summarization with SBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sentence-Bert to summarize the abstract (extractive summary)\n",
    "# https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models\n",
    "from summarizer.sbert import SBertSummarizer\n",
    "model = SBertSummarizer(model='paraphrase-multilingual-MiniLM-L12-v2') #all-MiniLM-L12-v2\n",
    "body = '''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. Hence, owing to the fact\n",
    "that it is a very innovative and, above all, safe solution, it is possible to determine the condition of the\n",
    "building, locate places exposed to thermal escape, and plan actions to improve the condition of the\n",
    "facility. The presented work is devoted to the technology of creating a dense point cloud and a 3D\n",
    "model, based on data obtained from UAV. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "summary = model(body, num_sentences=3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with paraphrase-multilingual v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "# with all-MiniLM-L12-v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera.'''\n",
    "# all-mpnet-base-v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "# paraphrase-multilingual-MiniLM-L12-v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA) with gensim for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = utils.LSI_LDA()\n",
    "texts_tokenized = LDA.preprocess(database, column='abstract') # ca. 5 min\n",
    "main_topics_lda = LDA.create_gensim_lda_model(categories='eCH') # ca. 3 min\n",
    "vis = LDA.prepare_plot_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic classification with TF-IDF vectors and predefined categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INSPIRE categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/INSPIRE_categories.csv\", sep=';', encoding='UTF-16')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0][8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eCH categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/eCH_categories.csv\", sep=';', encoding='UTF-16')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic classification with TF-IDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = utils.TFIDF_BM25()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "# prepare the categories for INSPIRE\n",
    "cats_index = df.index.values\n",
    "# translate from english to german because the german description is too short\n",
    "full_desc_DE = [utils.translate(element, lang='de', translator='google') for element in df['complete_description'].tolist()]\n",
    "description_title_DE = [df['category_DE'].values.tolist()[i]+', '+full_desc_DE[i]\n",
    "                     for i in range(0, len(df))]\n",
    "stemmed_categories = [utils.tokenize_abstract(text, output_scores=False, stem_words=True) for\n",
    "                       text in description_title_DE]\n",
    "joined_categories = [' '.join(words) for words in stemmed_categories]\n",
    "vectorizer = bm25.vectorizer\n",
    "vectorizer.fit(joined_categories)\n",
    "score_categories = super(TfidfVectorizer, vectorizer).transform(joined_categories)\n",
    "doc_length = score_categories.sum(1).A1\n",
    "avd = score_categories.sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_keywords = [utils.stemming_sentence(text) for text in [' '.join(words) for words in keywords_dataset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the database for the search (keywordds from rake in class NLP_spacy)\n",
    "# WARNING: the results aren't satisfactory\n",
    "import numpy as np\n",
    "dataset = 20\n",
    "k1 = 1.6\n",
    "b = 0.75\n",
    "for word in stemmed_keywords[dataset]:\n",
    "    query, = super(TfidfVectorizer, vectorizer).transform([word])\n",
    "    assert sparse.isspmatrix_csr(query)\n",
    "    kw = score_categories.tocsc()[:, query.indices]\n",
    "    idf = vectorizer._tfidf.idf_[None, query.indices] - 1.\n",
    "    scores = (kw.multiply(np.broadcast_to(idf, kw.shape)) * (k1 + 1)/ kw + (k1 * (1 - b + b * doc_length / avd))[:, None]).sum(1).A1\n",
    "    scores = [round(score, 2) for score in scores if score != 0.0 and not np.isnan(score)]\n",
    "    category = [cats_index[i] for i in range(0, len(scores)) if scores[i] > 0.0]\n",
    "    print(category, scores, word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_title_DE[17], description_title_DE[19], description_title_DE[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.loc[dataset]['abstract']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard/Cosine similarity -> for query search\n",
    "# WordNet-word-similarity -> for query search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add extracted data to raw data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract keywords from the abstract and add them to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword extraction\n",
    "NLP = utils.NLP_spacy()\n",
    "keywords_dataset = NLP.extract_refined_keywords(database, use_rake=True, refine_keywords=True,\n",
    "                                                 column='abstract', keyword_length=3, num_keywords=15)\n",
    "\n",
    "# Add the extracted keywords to the raw data dataframe\n",
    "def join_keywords(keywords_list):\n",
    "    keywords = ', '.join(kw for kw in keywords_list)\n",
    "    return keywords\n",
    "database_raw['keywords_nlp'] = list(map(join_keywords, keywords_dataset))\n",
    "# database_raw['keywords_rake'] = list(map(join_keywords, keywords_dataset))          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the abstracts with a SBert model and add them to the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = NLP.summarize_texts(database, column='abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summaries)\n",
    "database_raw['summary'] = summaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect and add the dataset's language based on titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'english':('EN', 'ENG'), 'french':('FR','FRA'), 'german':('DE','DEU'), 'italian':('IT','ITA'), 'not_found':('NA','NAN')}\n",
    "database_raw['lang_3'] = database_raw.apply(lambda row: language_dict[utils.detect_language(row['title'], not_found=True)][1], axis=1)\n",
    "database_raw['lang_2'] = database_raw.apply(lambda row: language_dict[utils.detect_language(row['title'], not_found=True)][0], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check metadata quality and save it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_raw = utils.check_metadata_quality(database_raw, search_word='nan',\n",
    "                                            search_columns=['abstract', 'keywords', 'metadata'],\n",
    "                                            case_sensitive=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the dataset for redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the str in the dataframe\n",
    "database_raw = database_raw.replace(to_replace=\"'\", value=\"-\", regex=True)\n",
    "database_raw = database_raw.replace(to_replace='\\\"', value=\"-\", regex=True)\n",
    "database_raw = database_raw.replace(to_replace=\"  \", value = \" \", regex=True)\n",
    "database_raw = database_raw.replace(to_replace=\"    \", value = \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database_raw.to_csv('rawdata_scraper.csv')\n",
    "database_raw.to_pickle('data/test_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_pickle('data/test_preprocessed.pkl')\n",
    "row = 201\n",
    "print(db['abstract'].loc[row])\n",
    "print(db['keywords_rake'].loc[row])\n",
    "print(db['keywords_nlp'].loc[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.read_pickle(\"data/merged_data.pkl\")\n",
    "# new_db = pd.read_csv(\"data/geoservices_CH.csv\", low_memory=False)\n",
    "# new_db = new_db.fillna(\"nan\")\n",
    "# new_db = new_db.replace(to_replace=\"'\", value=\"-\", regex=True)\n",
    "# new_db = new_db.replace(to_replace='\\\"', value=\"-\", regex=True)\n",
    "# new_db = new_db.replace(to_replace=\"  \", value = \" \", regex=True)\n",
    "# new_db = new_db.replace(to_replace=\"    \", value = \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "translate_columns = [\"keywords_nlp\"]\n",
    "for translate_column in translate_columns:\n",
    "    for lang in ['en','fr','it']:\n",
    "        tlang = time()\n",
    "        new_col = translate_column+'_'+lang\n",
    "        if translate_column == 'title':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_text(\n",
    "                row[translate_column],to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'abstract':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_abstract(\n",
    "                row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'keywords':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'keywords_nlp':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column].split(','), to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=True), axis=1)\n",
    "        else:\n",
    "            print(f\"Column {translate_column} could not be translated\")\n",
    "        print(f\"Transaltion of {translate_column} in {lang} required {(time()-tlang)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "translate_columns = [\"abstract\"]\n",
    "for translate_column in translate_columns:\n",
    "    for lang in ['it']:\n",
    "        tlang = time()\n",
    "        new_col = translate_column+'_'+lang\n",
    "        if translate_column == 'title':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_text(\n",
    "                row[translate_column],to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'abstract':\n",
    "            one_piece = ' | '.join(db.loc[0:10]['abstract'].to_list()).replace(\"| nan |\", \"\")\n",
    "            abstract_translated = utils.translate_abstract(one_piece,\n",
    "                                                           to_lang=lang, from_lang='DEU',\n",
    "                                                           use_api=True)\n",
    "            db[new_col] = abstract_translated.split(' | ')\n",
    "            # db[new_col] = db.progress_apply(lambda row: utils.translate_abstract(\n",
    "            #     row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "            #     use_api=True), axis=1)\n",
    "        elif translate_column == 'keywords':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'keywords_nlp':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column].split(','), to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=True), axis=1)\n",
    "        else:\n",
    "            print(f\"Column {translate_column} could not be translated\")\n",
    "        print(f\"Transaltion of {translate_column} in {lang} required {(time()-tlang)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(one_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.loc[10000:10200][['abstract','abstract_it']].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Abstract DE: {int(1280/60)} minutes\")\n",
    "print(f\"Abstract IT: {int(5750/60)} minutes\")\n",
    "print(f\"Abstract FR: {int(5092/60)} minutes\")\n",
    "print(f\"Abstract EN: {int(5141/60)} minutes\")\n",
    "\n",
    "print(f\"Title DE: {int(3738/60)} minutes\")\n",
    "print(f\"Title IT: {int(13998/60)} minutes\")\n",
    "print(f\"Title FR: {int(11284/60)} minutes\")\n",
    "print(f\"Title EN: {int(17649/60)} minutes\")\n",
    "\n",
    "print(f\"Keywords DE: {int(150/60)} minutes\")\n",
    "print(f\"Keywords IT: {int(1065/60)} minutes\")\n",
    "print(f\"Keywords FR: {int(1623/60)} minutes\")\n",
    "print(f\"Keywords EN: {int(1384/60)} minutes\")\n",
    "\n",
    "print(f\"Keywords_nlp DE: {int(4999/60)} minutes\")\n",
    "print(f\"Keywords_nlp IT: {int(16972/60)} minutes\")\n",
    "print(f\"Keywords_nlp FR: {int(19767/60)} minutes\")\n",
    "print(f\"Keywords_nlp EN: {int(24484/60)} minutes\")\n",
    "\n",
    "print(f\"\\nTotal ET: {(3738/60+13998/60+11284/60+17649/60+3738/60+13998/60+11284/60+17649/60+150/60+1065/60+1623/60+1384/60+4999/60+16972/60+19767/60+24484/60)/60} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transofrmers\n",
    "# sentencepiece\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\") #  google/mt5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.lang_code_to_id[\"de_DE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"The quick brown fox jumps over a lazy dog\"\n",
    "model_in = tokenizer(t, return_tensors=\"pt\")\n",
    "translated_tensors = model.generate(**model_in, forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"])\n",
    "translated_tokens = tokenizer.batch_decode(translated_tensors, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(translated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = utils.NLP_spacy()# missing keywords from abstract: 4803\n",
    "keywords_dataset = NLP.extract_refined_keywords(db[(db['keywords_nlp']=='')],\n",
    "                                                use_rake=True, refine_keywords=True,\n",
    "                                                column='title', keyword_length=3, num_keywords=15)\n",
    "k = 0\n",
    "for i, row in db[(db['keywords_nlp']=='')].iterrows():\n",
    "    if keywords_dataset[k]:\n",
    "        db.loc[i, 'keywords_nlp'] = \",\".join(keywords_dataset[k])\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"C:/Users/ma1021525/OneDrive - FHNW/Documents/Projects/git/Geoharvester/scraper/data/merged_data.pkl\"\n",
    "db = pd.read_pickle(pickle_path)\n",
    "# utils.translate(text, to_lang='de', translator='google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db[['provider','title','endpoint','metadata']][1500:1520].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in db[(db['lang_2']=='NA')&(db['provider']=='Bund')].iterrows():\n",
    "    db.loc[i, 'lang_2'] = 'DE'\n",
    "    db.loc[i, 'lang_3'] = 'DEU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'english':('EN', 'ENG'), 'french':('FR','FRA'), 'german':('DE','DEU'), 'italian':('IT','ITA'), 'not_found':('NA','NAN')}\n",
    "for i, row in db[(db['lang_2']=='NA')&(db['keywords']!='nan')].iterrows():\n",
    "    lang = utils.detect_language(row['keywords'], not_found=True)\n",
    "    if lang[0] != 'not_found':\n",
    "        lan = language_dict[lang]\n",
    "        db.loc[i, 'lang_2'] = lan[0]\n",
    "        db.loc[i, 'lang_3'] = lan[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator, exceptions\n",
    "\n",
    "def translate_text(text, to_lang, from_lang):\n",
    "    language_dict = {'ENG':'en', 'FRA':'fr', 'DEU':'de', 'ITA':'it','NAN':'na'}\n",
    "    if language_dict[from_lang] == to_lang:\n",
    "        return text\n",
    "    else:\n",
    "        try:\n",
    "            trnd = GoogleTranslator(source='auto', target=to_lang).translate(text.replace('_',' '))\n",
    "        except exceptions.TranslationNotFound:\n",
    "            trnd = 'nan'\n",
    "        return trnd\n",
    "\n",
    "def translate_abstract(text, to_lang, from_lang):\n",
    "    language_dict = {'ENG':'en', 'FRA':'fr', 'DEU':'de', 'ITA':'it','NAN':'na'}\n",
    "    if to_lang != language_dict[from_lang] and text != 'nan':\n",
    "        if not text.startswith('http') or text.startswith('Link zu Metadaten:'):\n",
    "            try:\n",
    "                trnd = GoogleTranslator(source='auto', target=to_lang).translate(text.replace('_',' '))\n",
    "            except exceptions.TranslationNotFound:\n",
    "                trnd = 'nan'\n",
    "            return trnd\n",
    "        else:\n",
    "            return 'nan'\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def translate_keywords(text, to_lang, from_lang):\n",
    "    if type(text) == str:\n",
    "        text = [text]\n",
    "    kwds = []\n",
    "    for kwd in text:\n",
    "        language_dict = {'ENG':'en', 'FRA':'fr', 'DEU':'de', 'ITA':'it','NAN':'na'}\n",
    "        if kwd != 'nan' and language_dict[from_lang] != to_lang:\n",
    "            if not kwd.startswith('http') or kwd.startswith('Link zu Metadaten:'):\n",
    "                try:\n",
    "                    kwd_trnsd = GoogleTranslator(source='auto', target=to_lang).translate(kwd.replace('_',' '))\n",
    "                    if not kwd_trnsd:\n",
    "                        kwd_trnsd = 'nan'\n",
    "                except exceptions.TranslationNotFound:\n",
    "                    kwd_trnsd = 'nan'\n",
    "            else:\n",
    "                kwd_trnsd = 'nan'\n",
    "            kwds.append(kwd_trnsd)\n",
    "        else:\n",
    "            kwds.append(kwd)\n",
    "    return ','.join(kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "languages = ['de']\n",
    "translate_column = 'keywords_nlp'#'title'#'abstract' # title # keywords\n",
    "\n",
    "for lang in languages:\n",
    "    print(f\"translating {translate_column} in {lang} ...\")\n",
    "    new_column = translate_column+'_'+lang\n",
    "    if translate_column == 'title':\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_text(row[translate_column],to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    elif translate_column == 'abstract':\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_abstract(row[translate_column], to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    elif translate_column == 'keywords':\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_keywords(row[translate_column], to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    else:\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_keywords(row[translate_column].split(','), to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    db.to_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'keywords'\n",
    "def count_links(field):\n",
    "    if field.startswith('http') or field.startswith('Link zu Metadaten:'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def count_nan(field):\n",
    "    if field == 'nan':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "db[f'{column}_link'] = db.progress_apply(lambda row: count_links(row[column]), axis=1)\n",
    "db[f'{column}_nan'] = db.progress_apply(lambda row: count_nan(row[column]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gr = db.groupby(['provider'], numeric_only=True).sum()[['abstract_link', 'abstract_nan','keywords_link','keywords_nan']]\n",
    "db_gr.rename({'Geodienste':'Geodnst'}, inplace=True)\n",
    "db_gr.sort_values(by=['provider'], ascending=False, inplace=True)\n",
    "db_gr['count'] = db.groupby(['provider']).count()[['title']].sort_values(by=['provider'], ascending=False)['title'].to_list()\n",
    "db_gr['abstract'] = 100 / db_gr['TITLE'] * db_gr['ABSTRACT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['abstract_link'])\n",
    "plt.title(\"Abstracts in % containing just a link\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['abstract_nan'])\n",
    "plt.title(\"Empty abstracts in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * (db_gr['abstract_link']+db_gr['abstract_nan']))\n",
    "plt.title(\"Unusable abstracts in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['keywords_link'])\n",
    "plt.title(\"Keywords in % containing just a link\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['keywords_nan'])\n",
    "plt.title(\"Empty keywords in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * (db_gr['keywords_link']+db_gr['keywords_nan']))\n",
    "plt.title(\"Unusable keywords in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://c8j9w8r3.rocketcdn.me/wp-content/uploads/2017/03/join-types-merge-names.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation ranking function\n",
    "- Results relevance evaluation\n",
    "- Results ranking evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'server'))\n",
    "from app.redis.methods import results_ranking, test_ranking\n",
    "\n",
    "db = pd.read_pickle('data/preprocessed_data.pkl')\n",
    "# db = db.rename(columns={'OWNER':'provider','TITLE':'title', 'NAME':'name','MAPGEO':'mapgeo','TREE':'tree','GROUP':'group',\n",
    "#                    'ABSTRACT':'abstract','KEYWORDS':'keywords','KEYWORDS_NLP':'keywords_nlp','LEGEND':'legend',\n",
    "#                    'CONTACT':'contact','ENDPOINT':'endpoint','METADATA':'metadata','UPDATE':'update','SERVICE':'service',\n",
    "#                    'MAX_ZOOM':'max_zoom','CENTER_LAT':'center_lat','CENTER_LON':'center_lon','BBOX':'bbox','SUMMARY':'summary',\n",
    "#                    'LANG_3':'lang_3','LANG_2':'lang_2','METAQUALITY':'metaquality'})\n",
    "# db = db.rename(columns={'SERVICETYPE':'servicetype','SERVICELINK':'servicelink'})\n",
    "# db.to_pickle('data/preprocessed_data.pkl')\n",
    "db = db[['provider','title','abstract','keywords','keywords_nlp','metaquality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time()\n",
    "db[(db['title'].str.contains('Winterthur', case=False)) | (db['keywords_nlp'].str.contains('winterthur', case=False))]# possiamo indicizzarlo essendo solo 6 keywords (verkehrsanordnung , städte zürich, kanton zürich, winterthur, tempo30, begegnungszonen'), mentre invece l'abstract di 33 parole viene diffice e le keywords normali sono vuote! (dataset: 17705)\n",
    "# print(round(time()-t0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time()\n",
    "db[(db['title'].str.contains('Winterthur'))]\n",
    "# print(round(time()-t0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mun_ch_15K = pd.read_csv('./data/mun_ch_15K.csv', header=None)\n",
    "mun_ch_15K.columns = ['municipality']\n",
    "mun_ch_15K['OWS_discovered'] = 0\n",
    "mun_ch_15K['OWS'] = 0\n",
    "for mun in mun_ch_15K['municipality'].to_list():\n",
    "    t = db[(db['title'].str.contains(mun, case=False)) | (db['keywords_nlp'].str.contains(mun, case=False))]\n",
    "    mun_ch_15K.loc[mun_ch_15K['municipality']==mun, ['OWS']]=len(db[(db['title'].str.contains(mun, case=False))])\n",
    "    ows = len(t[~t.index.isin(db[(db['title'].str.contains(mun, case=False))].index.to_list())])#.loc[5311]['abstract'] # Also in this case THUN we have 5729 that is not discovered \n",
    "    mun_ch_15K.loc[mun_ch_15K['municipality']==mun, ['OWS_discovered']]=ows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mun_ch_15K[mun_ch_15K['OWS_discovered']<200]['OWS_discovered'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mun_ch_15K[(mun_ch_15K['OWS_discovered']<200)].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[(db['abstract'] != 'nan') &\n",
    "   (db['abstract'] != 'n.a.')&\n",
    "   (~db['abstract'].str.contains('@'))&\n",
    "   (db['provider'] == 'Bund')]['keywords_nlp'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[453]['keywords_nlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[453]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the query in the database called db\n",
    "# combined_search use AND while not combined use OR between the words\n",
    "# it returns a pandas dataframe\n",
    "def build_query(query_list, query_fields, combined_search=True):\n",
    "    query_str = []\n",
    "    if len(query_list) == 1:\n",
    "        query_word = query_list[0]\n",
    "        for field in query_fields:\n",
    "            query_str.append(f\"(db['{field}'].str.contains('{query_word}', case=False))\")\n",
    "        query_str = '|'.join(query_str)\n",
    "    else:\n",
    "        if combined_search:\n",
    "            operator = '&'\n",
    "        else:\n",
    "            operator = '|'\n",
    "        for query_word in query_list:\n",
    "            query_part = []\n",
    "            for field in query_fields:\n",
    "                if field == 'keywords_nlp':\n",
    "                    query_word = query_word.lower()\n",
    "                query_part.append(f\"(db['{field}'].str.contains('{query_word}', case=False))\")\n",
    "            query_str.append('|'.join(query_part))\n",
    "        query_str = operator.join(query_str)\n",
    "    print(query_str)\n",
    "    return eval(f\"db[{query_str}]\")\n",
    "\n",
    "def evaluate_search_results(sorted_results, test_index):\n",
    "    scores = []\n",
    "    for idx in test_index:\n",
    "        if idx in sorted_results.index.to_list():\n",
    "            scores.append(sorted_results.index.to_list().index(idx))\n",
    "        else:\n",
    "            scores.append(100)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test 1: query_words = ['drohne','einschränkung']; test_rows = [20263, 20806, 23296]\n",
    "- Test 2: query_words = ['durchlässigkeit','Deckschichten']; test_rows = [536, 537]\n",
    "- Test 3: query_words = ['Amphibienzugstelle','fotopunkte']; test_rows = [634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644]\n",
    "- Test 4: query_words = ['radweg','zürich']; test_rows = [17557]\n",
    "- Test 4b: query_words = ['radweg','kanton schwyz']; test_rows = [11719, 12365]\n",
    "- Test 4c: query_words = ['radweg','schweiz']; test_rows = [19936, 21190]\n",
    "- Test 4d: query_words = ['veloweg','schweiz']; test_rows = [19936, 21190]\n",
    "- Test 5: query_words = ['berufsinformationszentren','kanton bern']; test_rows = [5347,5348,5349,5350]\n",
    "- Test 6: query_words = ['roemisch','pfosten','augusta raurica']; test_rows = [1781]\n",
    "- Test 7: query_words = ['wildtierkorridore','schweiz']; test_rows = [19977, 20939, 11829, 12499]\n",
    "- Test 7b: query_words = ['wildtierkorridore','kanton solothurn']; test_rows = [10398]\n",
    "- Test 8: query_words = ['Abteilung Wasserbau','Bewilligung', 'Zürich']; test_rows = [17757]\n",
    "- Test 9: query_words = ['bezirke','kanton zürich']; test_rows = [17764, 17863, 36651]\n",
    "- Test 10: query_words = ['rohstoffe','kanton schaffhausen']; test_rows = [10321]\n",
    "- Test 10b: query_words = ['rohstoffe','schweiz']; test_rows = [20701, 20702, 20698, 20700, 20412,20696,20699,20411]\n",
    "- Test 11: query_words = ['eignung','solarenergie','schweiz']; test_rows = [20289, 20790, 20290, 20791]\n",
    "- Test 11b: query_words = ['eignung','solarenergie','kanton aargau']; test_rows = [364, 365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_words = ['CH-151.1']\n",
    "test_rows = [634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644]\n",
    "query_fields = ['title','keywords','keywords_nlp', 'abstract'] # all possible results\n",
    "print(build_query(query_words,query_fields,combined_search=True).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_fields = [\"title\", \"keywords\"]# first list exact match, second list contains\n",
    "sum_score = True\n",
    "\n",
    "if sum_score:\n",
    "    for i in range(len(sorting_fields)):\n",
    "        test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],sorting_fields[0:i+1]])\n",
    "        print(np.asarray(evaluate_search_results(test_db, test_rows)).sum(), sorting_fields[0:i+1])\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],['keywords']])\n",
    "    print(np.asarray(evaluate_search_results(test_db, test_rows)).sum(), 'keywords')\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words)\n",
    "    print(np.asarray(evaluate_search_results(test_db, test_rows)).sum(), 'custom GeoHarvester')\n",
    "else:\n",
    "    for i in range(len(sorting_fields)):\n",
    "        test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],sorting_fields[0:i+1]])\n",
    "        print(evaluate_search_results(test_db, test_rows), sorting_fields[0:i+1])\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],['keywords']])\n",
    "    print(evaluate_search_results(test_db, test_rows), 'keywords')\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words)\n",
    "    print(evaluate_search_results(test_db, test_rows), 'custom GeoHarvester')\n",
    "print(f\"Number of test rows: {len(test_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.asarray([12, 9, 16, 8, 1, 17, 0, 14, 11, 13, 10]).sum(), np.asarray([10, 3, 8, 7, 0, 6]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_db[0:100].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_new_data(db, translate_column, languages, one_shot=True):\n",
    "    \"\"\"\n",
    "    Translates the preprocessed data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db : df\n",
    "        Dataframe to be translated\n",
    "    translate_column : string\n",
    "        Column name from the list of columns to translate as defined in WORKFLOW_TRANSLATE_COLUMNS\n",
    "    languages : list\n",
    "        Language to translate into, defined by LANG_FROM_PIPELINE\n",
    "    one_shot : bool\n",
    "        if true, it will translate all the data in one shot\n",
    "\n",
    "    Output \n",
    "    ----------\n",
    "    <language_abbr>_translated.pkl : pickle\n",
    "        Outputs a pickle file of the translation which is uploaded as artifact to github\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: process the one shot translation in language chunks (datasets can have differenz languages)\n",
    "\n",
    "    db = db.fillna(\"nan\")\n",
    "    for lang in languages:\n",
    "        new_col = translate_column+'_'+lang\n",
    "        if not one_shot:\n",
    "            if translate_column == 'title':\n",
    "                tlang1 = time()\n",
    "                db[new_col] = db.apply(lambda row: utils.translate_text(\n",
    "                    row[translate_column],to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "                tlang2 = time()\n",
    "                print(f\"Processed 'Title' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "            elif translate_column == 'abstract':\n",
    "                tlang1 = time()\n",
    "                db[new_col] = db.apply(lambda row: utils.translate_abstract(\n",
    "                    row[translate_column], to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "                tlang2 = time()\n",
    "                print(f\"Processed 'Abstract' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "            elif translate_column == 'keywords':\n",
    "                tlang1 = time()\n",
    "                db[new_col] = db.apply(lambda row: utils.translate_keywords(\n",
    "                    row[translate_column], to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "                tlang2 = time()\n",
    "                print(f\"Processed 'Keywords' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "            elif translate_column == 'keywords_nlp':\n",
    "                tlang1 = time()\n",
    "                db[new_col] = db.apply(lambda row: utils.translate_keywords(\n",
    "                    row[translate_column].split(','), to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "                tlang2 = time()\n",
    "                print(f\"Processed 'Keywords_NLP' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "            else:\n",
    "                print(f\"Column {translate_column} could not be translated\")\n",
    "        else:\n",
    "            separator = ''\n",
    "            chunk_size = 200\n",
    "            for i in range(int(len(db)/chunk_size)+1):\n",
    "                while utils.check_length_text(' | '.join(db[i*chunk_size:chunk_size*(i+1)][translate_column].to_list())):\n",
    "                    chunk_size = int(chunk_size/2)\n",
    "            print(f\"Set chunk size to {chunk_size}\")\n",
    "\n",
    "            translated_chunks = []\n",
    "            for i in range(int(len(db)/chunk_size)+1):\n",
    "                if db[i*chunk_size:chunk_size*(i+1)][translate_column].empty:\n",
    "                    continue\n",
    "                col_oncie = separator.join(db[i*chunk_size:chunk_size*(i+1)][translate_column].to_list())\n",
    "                if translate_column == 'title':\n",
    "                    tlang1 = time()\n",
    "                    title_oncie_trnsd = utils.translate_text(col_oncie.replace('_',' '), to_lang=lang, from_lang='NAN')\n",
    "\n",
    "                    if len(col_oncie.split(separator)) != len(title_oncie_trnsd.split(separator)):\n",
    "                        print(col_oncie)\n",
    "                        print(title_oncie_trnsd)\n",
    "                        print('--')\n",
    "                    \n",
    "                    translated_chunks.extend(title_oncie_trnsd.split(separator))\n",
    "                    tlang2 = time()\n",
    "                    # print(f\"Processed 'Title' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "                elif translate_column == 'abstract':\n",
    "                    tlang1 = time()\n",
    "                    abstract_oncie_trnsd = utils.translate_abstract(col_oncie, to_lang=lang, from_lang='NAN')\n",
    "                    translated_chunks.extend(abstract_oncie_trnsd.split(separator))\n",
    "                    tlang2 = time()\n",
    "                    # print(f\"Processed 'Abstract' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "                elif translate_column == 'keywords':\n",
    "                    tlang1 = time()\n",
    "                    keywords_oncie_trnsd = utils.translate_keywords(col_oncie, to_lang=lang, from_lang='NAN')\n",
    "                    translated_chunks.extend(keywords_oncie_trnsd.split(separator))\n",
    "                    tlang2 = time()\n",
    "                    # print(f\"Processed 'Keywords' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "                elif translate_column == 'keywords_nlp':\n",
    "                    tlang1 = time()\n",
    "                    keywords_nlp_oncie_trnsd = utils.translate_keywords(col_oncie, to_lang=lang, from_lang='NAN')\n",
    "                    translated_chunks.extend(keywords_nlp_oncie_trnsd.split(separator))\n",
    "                    tlang2 = time()\n",
    "                    # print(f\"Processed 'Keywords_NLP' in {lang} {round(tlang2-tlang1)} s'\")\n",
    "                else:\n",
    "                    print(f\"Column {translate_column} could not be translated\")\n",
    "            db[new_col] = translated_chunks\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set chunk size to 50\n",
      "Gewässerschutzkarte Gewässerschutzbereiche bezeichnet)(Grundwasserkarte Mittelwasser)(Grundwasserkarte Grundwasserstand Mittelwasser)(Grundwasserkarte Grundwassermächtigkeit Mittelwasser)(Grundwasserkarte Flurabstand Mittelwasser)(Grundwasserkarte Niedrigwasser)(Grundwasserkarte Grundwasserstand Niedrigwasser)(Grundwasserkarte Grundwassermächtigkeit Niedrigwasser)(Grundwasserkarte Flurabstand Niedrigwasser)(Grundwasserkarte Hochwasser)(Grundwasserkarte Grundwassermächtigkeit Hochwasser)(Grundwasserkarte Grundwasserstand Hochwasser)(Grundwasserkarte Flurabstand Hochwasser)(Grundwasserkarte Grundwasserbereich)(Grundwasserkarte Lockergestein)(Werkkataster Abwasser Übersichtsplan)(Werkkataster Abwasser Werkplan)(Lebensraum Gesamtbewertung)(Sohleverbauung)(Uferbreite links)(Uferbreite rechts)(Verbauung Böschungsfuss links)(Verbauung Böschungsfuss rechts)(Wanderhindernisse Bauwerke)(Wanderhindernisse Absturz)(Wasserspiegel Breitenvariabilität)(Fischarten Verbreitungskarte Bachforelle)(Fischarten Verbreitungskarte Groppe)(Fischarten Verbreitungskarte Elritze)(Fischarten Verbreitungskarte Schmerle)(Fischarten Verbreitungskarte Aesche)(Fischarten Verbreitungskarte Strömer)(Fischarten Verbreitungskarte Schneider)(Fischarten Verbreitungskarte Alet)(Fischarten Verbreitungskarte Barbe)(Fischarten Verbreitungskarte Nase)(Fischarten Verbreitungskarte Hasel)(Fischarten Verbreitungskarte Gründling)(Fischarten Verbreitungskarte Rotauge)(Fischarten Verbreitungskarte Rotfeder)(Fischarten Verbreitungskarte Aal)(Fischarten Verbreitungskarte Exoten)(Fischfauna Altersstruktur)(Fischfauna Gesamtbeurteilung)(Fischfauna Artenzusammensetzung)(Fischfauna Besiedlungsdichte)(Fischfauna Dominanzverhältnis)(Fischfauna krankhafte Veränderungen)(Makrofauna Gesamtbeurteilung (Messw. Fühling))(Makrofauna Gesamtbeurteilung (Messw. Sommer)\n",
      "Water protection map of water protection areas)(groundwater map of mean water)(groundwater map of groundwater level of mean water)(groundwater map of groundwater thickness of mean water)(groundwater map of surface depth of mean water)(groundwater map of low water)(groundwater map of groundwater level of low water)(groundwater map of groundwater thickness of low water)(groundwater map of surface depth of low water)(groundwater map of surface depth of high water)(groundwater map of groundwater thickness of high water)(groundwater map of groundwater level of high water)(groundwater map of surface depth of high water)(groundwater map of groundwater area)(groundwater map of loose rock)(works cadastre of wastewater overview plan)(works cadastre of wastewater work plan)(habitat overall assessment)(bed construction)(bank width on the left)(bank width on the right)(construction of slope foot on the left)(construction of slope foot on the right)(immigration obstacles, structures)(immigration obstacles, falls)(water level width variability)(fish species distribution map of brown trout)(fish species distribution map Bullhead)(Fish species distribution map minnow)(Fish species distribution map loach)(Fish species distribution map grayling)(Fish species distribution map bream)(Fish species distribution map roach)(Fish species distribution map dace)(Fish species distribution map gudgeon)(Fish species distribution map roach)(Fish species distribution map rudd)(Fish species distribution map eel)(Fish species distribution map exotics)(Fish fauna age structure)(Fish fauna overall assessment)(Fish fauna species composition)(Fish fauna population density)(Fish fauna dominance ratio)(Fish fauna pathological changes)(Macrofauna overall assessment (measurements spring))(Macrofauna overall assessment (measurements summer)\n",
      "--\n",
      "Wasserbaukonzept Baulicher Hochwasserschutz)(Wasserbaukonzept Ausdolung)(Wasserbaukonzept Revitalisierung Aue)(Wasserbaukonzept Revitalisierung Gewässer)(Wasserbaukonzept Revitalisierung Sohle)(Wasserbaukonzept Überflutungs- und Retentionsgebiet)(Wasserbaukonzept Längsvernetzung)(Wasserbaukonzept Baulicher Hochwasserschutz umgesetzt)(Wasserbaukonzept Ausdolung umgesetzt)(Wasserbaukonzept Revitalisierung Aue umgesetzt)(Wasserbaukonzept Revitalisierung Gewässer umgesetzt)(Wasserbaukonzept Revitalisierung Sohle umgesetzt)(Wasserbaukonzept Überflutungs- und Retentionsgebiet umgesetzt)(Wasserbaukonzept Längsvernetzung umgesetzt)(Gewässernetz Hauptgewässer)(Gewässernetz)(Ungefähre Lage Übergangsbestimmungen GSchV)(Verzicht auf Gewässerraum (Linien))(Verzicht auf Gewässerraum (Flächen))(Gewässerraum nach Art. 36a GSchG)(Verzicht auf Gewässerraum (Linien), laufende Änderungen)(Verzicht auf Gewässerraum (Flächen), laufende Änderungen)(Gewässerraum nach Art. 36a GSchG)(Längsvernetzung - Bauwerke)(Längsvernetzung - Schwellen)(Massnahmentyp)(Längsvernetzung)(Fliessgewässer)(Siedlungsentwässerung)(Siedlungsentwässerung Deklaration Wertreduktion)(waermeverbundkataster_karte_und_infoabfrage)(Wärmeverbundkataster (Perimeter erweiterbare Wärmeverbunde) (transparent, nur für Infoabfrage))(Wärmeverbundkataster (Perimeter erweiterbare Wärmeverbunde))(Verkehrszählstellen MIV)(Verkehrszählstellen Velo)(Archäologische Schutzzonen (Flächen))(Augusta Raurica Mauern, nicht römisch (Flächen))(Augusta Raurica Mauern Umrandung, nicht römisch (Flächen))(Augusta Raurica Mauern, nicht römisch (Linien))(Augusta Raurica Mauern, römisch (Flächen))(Augusta Raurica Mauern Umrandung (keine,ausg,gpr,fb), römisch (Flächen))(Augusta Raurica Mauern, römisch (Linien))(Augusta Raurica Strassen, römisch)(Augusta Raurica Strassen, römisch Umrandung (keine,ausg,gpr,fb,lpr))(Augusta Raurica Strassen, nicht römisch)(Augusta Raurica Strassen, nicht römisch Umrandung (keine,ausg,gpr,fb,lpr))(August Raurica Grabungsverzeichnis)(Augusta Raurica Grabungsverzeichnis)(August Raurica Insulae)(August Raurica Insulae Text\n",
      "Hydraulic engineering concept for structural flood protection)(Hydraulic engineering concept for excavation)(Hydraulic engineering concept for revitalization of floodplains)(Hydraulic engineering concept for revitalization of water bodies)(Hydraulic engineering concept for revitalization of bed)(Hydraulic engineering concept for flood and retention areas)(Hydraulic engineering concept for longitudinal networking)(Hydraulic engineering concept for structural flood protection implemented)(Hydraulic engineering concept for excavation implemented)(Hydraulic engineering concept for revitalization of floodplains implemented)(Hydraulic engineering concept for revitalization of bed implemented)(Hydraulic engineering concept for flood and retention areas implemented)(Hydraulic engineering concept for longitudinal networking implemented)(Water network for main water bodies)(Water network)(Approximate location of transitional provisions of the GSchV)(Waiver of water area (lines))(Waiver of water area (areas))(Water area according to Art. 36a GSchG)(Waiver of water area (lines), ongoing changes)(Waiver of water area (areas), ongoing Changes)(Water area according to Art. 36a GSchG)(Longitudinal network - structures)(Longitudinal network - thresholds)(Type of measure)(Longitudinal network)(Watercourses)(Urban drainage)(Urban drainage declaration value reduction)(Heat network cadastre map and information query)(Heat network cadastre (Perimeter expandable heat networks) (transparent, only for information query))(Heat network cadastre (Perimeter expandable heat networks))(Traffic counting points MIV)(Traffic counting points bicycle)(Archaeological protection zones (areas))(Augusta Raurica walls, not Roman (areas))(Augusta Raurica walls border, not Roman (areas))(Augusta Raurica walls, not Roman (lines))(Augusta Raurica walls, Roman (areas))(Augusta Raurica walls border (none, ausg, gpr, fb), Roman (areas))(Augusta Raurica walls, Roman (lines))(Augusta Raurica streets, Roman)(Augusta Raurica streets, Roman border (none, ausg, gpr, fb, lpr))(Augusta Raurica streets, not Roman)(Augusta Raurica streets, not Roman border (none, ausg, gpr, fb, lpr))(August Raurica excavation list)(Augusta Raurica excavation list)(August Raurica Insulae)(August Raurica Insulae text\n",
      "--\n",
      "Ungedeckter Veloabstellplatz)(Gefahr (Abfrage))(Einbahn (Abfrage))(Steigung (Abfrage))(gut befahrbares Velonetz (Abfrage))(sonstige für Velos geeignete Strassen (Abfrage))(Schiebestrecke (Abfrage))(Fussgängerzone (Abfrage))(EuroVelo 5)(EuroVelo 5 (Symbol))(EuroVelo 6)(EuroVelo 6 (Symbol))(EuroVelo 15)(EuroVelo 15 (Symbol))(SchweizMobil 2)(SchweizMobil 2 (Symbol))(SchweizMobil 3)(SchweizMobil 3 (Symbol))(SchweizMobil 7)(SchweizMobil 7 (Symbol))(SchweizMobil 23)(SchweizMobil 23 (Symbol))(SchweizMobil 97 - Dreiland-Radweg)(SchweizMobil 97 - Dreiland-Radweg (Symbol))(SchweizMobil 979 - Kleiner Dreiland-Radweg)(SchweizMobil 979 - Kleiner Dreiland-Radweg (Symbol))(SchweizMobil Bike 3)(SchweizMobil Bike 3 (Symbol))(Südschwarzwald-Radweg)(Südschwarzwald-Radweg (Symbol))(alle touristischen Velorouten)(Alltagsvelorouten)(Handpumpe)(Kompressor)(Unbekannt)(Begegnungszone)(Fussgängerzone)(Kernzone Verkehrskonzept Innenstadt)(Tempo 30 - Zone)(Schleuder- oder Selbstunfall)(Überholunfall oder Fahrstreifenwechsel)(Auffahrunfall)(Abbiegeunfall)(Einbiegeunfall)(Überqueren der Fahrbahn)(Frontalkollision)(Parkierunfall)(Fussgängerunfall)(Tierunfall)(Andere\n",
      "Uncovered bicycle parking space)(Danger (query))(One-way street (query))(Gradient (query))(Easy to use bicycle network (query))(Other roads suitable for bicycles (query))(Pedestrian zone (query))(EuroVelo 5)(EuroVelo 5 (symbol))(EuroVelo 6)(EuroVelo 6 (symbol))(EuroVelo 15)(EuroVelo 15 (symbol))(SwitzerlandMobility 2)(SwitzerlandMobility 2 (symbol))(SwitzerlandMobility 3)(SwitzerlandMobility 3 (symbol))(SwitzerlandMobility 7)(SwitzerlandMobility 7 (symbol))(SwitzerlandMobility 23)(SwitzerlandMobility 23 (symbol))(SwitzerlandMobility 97 - Three-country cycle path)(SwitzerlandMobility 97 - Three-country cycle path (Symbol))(SwitzerlandMobility 979 - Small Three-Country Cycle Route)(SwitzerlandMobility 979 - Small Three-Country Cycle Route (Symbol))(SwitzerlandMobility Bike 3)(SwitzerlandMobility Bike 3 (Symbol))(Southern Black Forest Cycle Route)(Southern Black Forest Cycle Route (Symbol))(all tourist cycle routes)(Everyday cycle routes)(Hand pump)(Compressor)(Unknown)(Meeting zone)(Pedestrian zone)(Core zone of the inner city traffic concept)(30 km/h zone)(Skidding or self-driving accident)(Overtaking accident or lane change)(Rear-end collision)(Turning accident)(Turning in accident)(Crossing the road)(Head-on collision)(Parking accident)(Pedestrian accident)(Animal accident)(Other\n",
      "--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m prep_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/de_preprd_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m transalted_data \u001b[38;5;241m=\u001b[39m translate_new_data(prep_data[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10000\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, languages\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn [13], line 69\u001b[0m, in \u001b[0;36mtranslate_new_data\u001b[1;34m(db, translate_column, languages, one_shot)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m translate_column \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     68\u001b[0m     tlang1 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 69\u001b[0m     title_oncie_trnsd \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_oncie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNAN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(col_oncie\u001b[38;5;241m.\u001b[39msplit(separator)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(title_oncie_trnsd\u001b[38;5;241m.\u001b[39msplit(separator)):\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28mprint\u001b[39m(col_oncie)\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\OneDrive - FHNW\\Documents\\Projects\\git\\Geoharvester\\scraper\\utils.py:77\u001b[0m, in \u001b[0;36mtranslate_text\u001b[1;34m(text, to_lang, from_lang)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m         trnd \u001b[38;5;241m=\u001b[39m \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_lang\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m         trnd \u001b[38;5;241m=\u001b[39m trnd\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTranslationNotFound:\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\deep_translator\\google.py:67\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayload_key:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayload_key] \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m---> 67\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\urllib3\\response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\urllib3\\response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "prep_data = pd.read_pickle('data/de_preprd_data.pkl')\n",
    "transalted_data = translate_new_data(prep_data[0:10000], 'title', languages=['en','it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"C:/Users/ma1021525/Downloads/response_1711280247689.json\") as file:\n",
    "    json_data = json.load(file)\n",
    "json_string = json.dumps(json_data)\n",
    "try:\n",
    "    df = pd.read_json(json_string)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "un_char = set()\n",
    "for char in json_string:\n",
    "    try:\n",
    "        char.encode('utf-8')\n",
    "    except UnicodeEncodeError:\n",
    "        un_char.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_data), len(database_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = database_raw.replace(to_replace=\"'\", value=\"-\", regex=True)\n",
    "new_db = new_db.replace(to_replace='\\\"', value=\"-\", regex=True)\n",
    "new_db = new_db.replace(to_replace=\"  \", value = \" \", regex=True)\n",
    "new_db = new_db.replace(to_replace=\"    \", value = \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_preprocessing = new_db.merge(old_data, on=['name','title','provider','keywords','abstract','endpoint'], how='left',\n",
    "                                    indicator='_lmerge', suffixes=(None, \"_drop\"))\n",
    "to_preprocessing = to_preprocessing.loc[to_preprocessing['_lmerge']=='left_only']\n",
    "to_preprocessing = to_preprocessing[new_db.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = old_data.merge(new_db, on=['name','title','provider','keywords','abstract','endpoint'], how='inner', indicator='_innermerge',\n",
    "                           suffixes=(None,\"_drop\"))\n",
    "to_keep = to_keep[old_data.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_preprocessing), len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cog.torque.Graph at 0x2494445ed30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cog.torque import Graph\n",
    "g = Graph(\"translation\", cog_home=\"cog_home\")\n",
    "g.put(\"Gefahrenkarte\",\"means\",\"carta dei pericoli\")\n",
    "g.put(\"Gefahrenkarte\", \"means\",\"carte des dangers\")\n",
    "g.put(\"Gefahrenkarte\", \"means\",\"risk map\")\n",
    "g.put(\"carta dei pericoli\", \"means\", \"gefahrenkarte\")\n",
    "g.put(\"risk map\", \"synonym\",\"hazard map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "  <iframe srcdoc='\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "  <head>\n",
       "    <title>Cog Graph</title>\n",
       "    <style type=\"text/css\">\n",
       "       body {\n",
       "        padding: 0;\n",
       "        margin: 0;\n",
       "        width: 100%;!important; \n",
       "        height: 100%;!important; \n",
       "      }\n",
       "\n",
       "      #cog-graph-view {\n",
       "        width: 700px;\n",
       "        height: 700px;\n",
       "      }\n",
       "    </style>\n",
       "\n",
       "\n",
       "    <script\n",
       "      type=\"text/javascript\"\n",
       "      src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\"\n",
       "    ></script>\n",
       "  </head>  \n",
       " \n",
       "  <body>\n",
       "    <div id=\"cog-graph-view\"></div>\n",
       "\n",
       "    <script type=\"text/javascript\">\n",
       "\n",
       "    results =[{\"id\": \"hazard map\", \"from\": \"risk map\", \"to\": \"hazard map\"}, {\"id\": \"carte des dangers\", \"from\": \"Gefahrenkarte\", \"to\": \"carte des dangers\"}, {\"id\": \"risk map\", \"from\": \"Gefahrenkarte\", \"to\": \"risk map\"}, {\"id\": \"carta dei pericoli\", \"from\": \"Gefahrenkarte\", \"to\": \"carta dei pericoli\"}] \n",
       "\n",
       "    var nodes = new vis.DataSet();\n",
       "    var edges = new vis.DataSet();\n",
       "    for (let i = 0; i < results.length; i++) {\n",
       "        res = results[i];\n",
       "        nodes.update({\n",
       "            id: res.from,\n",
       "            label: res.from\n",
       "        });\n",
       "        nodes.update({\n",
       "            id: res.to,\n",
       "            label: res.to\n",
       "        });\n",
       "        edges.update({\n",
       "            from: res.from,\n",
       "            to: res.to\n",
       "        });\n",
       "\n",
       "    }\n",
       "\n",
       "    var container = document.getElementById(\"cog-graph-view\");\n",
       "    var data = {\n",
       "        nodes: nodes,\n",
       "        edges: edges,\n",
       "    };\n",
       "    var options = {\n",
       "        nodes: {\n",
       "            font: {\n",
       "                size: 20,\n",
       "                color: \"black\"\n",
       "            },\n",
       "            color: \"#46944f\",\n",
       "            shape: \"dot\",\n",
       "            widthConstraint: 200,\n",
       "\n",
       "        },\n",
       "        edges: {\n",
       "            font: \"12px arial #ff0000\",\n",
       "            scaling: {\n",
       "                label: true,\n",
       "            },\n",
       "            shadow: true,\n",
       "            smooth: true,\n",
       "            arrows: { to: {enabled: true}}\n",
       "        },\n",
       "        physics: {\n",
       "            barnesHut: {\n",
       "                gravitationalConstant: -30000\n",
       "            },\n",
       "            stabilization: {\n",
       "                iterations: 1000\n",
       "            },\n",
       "        }\n",
       "\n",
       "    };\n",
       "    var network = new vis.Network(container, data, options);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\n",
       "' width=\"700\" height=\"700\"> </iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g.v().tag(\"from\").out().tag(\"to\").view(\"means\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': [{'id': 'carte des dangers'},\n",
       "  {'id': 'risk map'},\n",
       "  {'id': 'carta dei pericoli'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.v().filter(func=lambda x: x.startswith(\"carta dei pericoli\")).out().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.v(\"carta dei pericoli\").out().all()['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carte des dangers\n",
      "risk map\n",
      "  hazard map\n",
      "carta dei pericoli\n"
     ]
    }
   ],
   "source": [
    "for result in g.v(\"Gefahrenkarte\").out().all()['result']:\n",
    "    print(result['id'])\n",
    "    if len(g.v(result['id']).out().all()['result']) > 0:\n",
    "        for res in g.v(result['id']).out().all()['result']:\n",
    "            print(\" \",res['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoharvester",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c031072c482ff12c1b8627010cd074fb6af482daeb686bcc3fbabf1c7aa304d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
